"""
Auto-generated by nifi2py from: {{ flow_name }}
Generated: {{ timestamp }}
Source: {{ template_name }}

IMPORTANT: This is generated code. Manual modifications may be overwritten.

Statistics:
- Processors: {{ processor_count }}
- Connections: {{ connection_count }}
- Stub processors: {{ stub_count }}
- Coverage: {{ coverage_percentage }}%
"""

from dataclasses import dataclass, field
from typing import Dict, List, Optional
import logging
import hashlib
import uuid as uuid_module
from datetime import datetime
{% for import_stmt in additional_imports %}
{{ import_stmt }}
{% endfor %}

logger = logging.getLogger(__name__)

# ============================================================================
# FlowFile Model
# ============================================================================

@dataclass
class FlowFile:
    """
    Represents a NiFi FlowFile - the fundamental unit of data flow.

    A FlowFile consists of:
    - Content: The actual data (bytes)
    - Attributes: Metadata key-value pairs
    - UUID: Unique identifier
    """
    content: bytes = b""
    attributes: Dict[str, str] = field(default_factory=dict)
    uuid: str = field(default_factory=lambda: str(uuid_module.uuid4()))

    @property
    def size(self) -> int:
        """Return the size of the content in bytes."""
        return len(self.content)

    @property
    def content_hash(self) -> str:
        """Return SHA-256 hash of the content."""
        return hashlib.sha256(self.content).hexdigest()

    def clone(
        self,
        content: Optional[bytes] = None,
        attributes: Optional[Dict[str, str]] = None,
    ) -> "FlowFile":
        """
        Create a clone of this FlowFile with optional overrides.

        Args:
            content: New content (if None, uses current content)
            attributes: New attributes (if None, copies current attributes)

        Returns:
            New FlowFile instance with new UUID
        """
        return FlowFile(
            content=content if content is not None else self.content,
            attributes=attributes if attributes is not None else self.attributes.copy(),
        )

    def update_attributes(self, **kwargs: str) -> "FlowFile":
        """
        Update attributes in-place and return self for chaining.

        Args:
            **kwargs: Attribute key-value pairs to add/update

        Returns:
            Self for method chaining
        """
        self.attributes.update(kwargs)
        return self

    def get_attribute(self, key: str, default: str = "") -> str:
        """
        Get an attribute value with optional default.

        Args:
            key: Attribute key
            default: Default value if key not found

        Returns:
            Attribute value or default
        """
        return self.attributes.get(key, default)


# ============================================================================
# Helper Functions (Expression Language Support)
# ============================================================================

{% for helper_func in helper_functions %}
{{ helper_func }}

{% endfor %}

# ============================================================================
# Processor Functions
# ============================================================================

{% for processor_func in processor_functions %}
{{ processor_func }}

{% endfor %}

# ============================================================================
# Flow Execution
# ============================================================================

# Connection graph: processor_id -> [(relationship, destination_processor_id), ...]
CONNECTIONS: Dict[str, List[tuple]] = {
{% for proc_id, connections in connection_graph.items() %}
    "{{ proc_id }}": [{% for rel, dest_id in connections %}("{{ rel }}", "{{ dest_id }}"){% if not loop.last %}, {% endif %}{% endfor %}],
{% endfor %}
}

# Processor mapping: processor_id -> function
PROCESSOR_MAP: Dict[str, callable] = {
{% for proc_id, func_name in processor_map.items() %}
    "{{ proc_id }}": {{ func_name }},
{% endfor %}
}

# Processor metadata for debugging
PROCESSOR_METADATA: Dict[str, Dict[str, str]] = {
{% for proc_id, metadata in processor_metadata.items() %}
    "{{ proc_id }}": {
        "name": "{{ metadata.name }}",
        "type": "{{ metadata.type }}",
        "is_stub": {{ metadata.is_stub }},
    },
{% endfor %}
}


def run_flow(flowfiles: List[FlowFile], start_processor_id: str, max_iterations: int = 10000) -> List[FlowFile]:
    """
    Execute the flow starting from a given processor.

    This uses breadth-first traversal with cycle detection to prevent infinite loops.

    Args:
        flowfiles: Input flowfiles
        start_processor_id: ID of starting processor
        max_iterations: Maximum iterations to prevent infinite loops (default: 10000)

    Returns:
        List of output flowfiles from terminal processors

    Raises:
        ValueError: If max_iterations exceeded (potential infinite loop)
    """
    queue = [(start_processor_id, flowfiles)]
    output_files = []
    iteration = 0

    # Track flowfiles through processors to detect cycles
    flowfile_processor_visits: Dict[str, int] = {}

    while queue:
        iteration += 1
        if iteration > max_iterations:
            raise ValueError(
                f"Maximum iterations ({max_iterations}) exceeded. "
                f"Possible infinite loop detected. Queue size: {len(queue)}"
            )

        proc_id, files = queue.pop(0)

        if proc_id not in PROCESSOR_MAP:
            logger.warning(f"Processor {proc_id} not found in PROCESSOR_MAP, skipping")
            # Still add to outputs as this might be an external endpoint
            output_files.extend(files)
            continue

        processor_func = PROCESSOR_MAP[proc_id]
        processor_meta = PROCESSOR_METADATA.get(proc_id, {})

        logger.debug(
            f"Processing {len(files)} flowfile(s) through {processor_meta.get('name', proc_id)} "
            f"({processor_meta.get('type', 'unknown')})"
        )

        for flowfile in files:
            # Track visits to detect cycles
            visit_key = f"{flowfile.uuid}:{proc_id}"
            flowfile_processor_visits[visit_key] = flowfile_processor_visits.get(visit_key, 0) + 1

            if flowfile_processor_visits[visit_key] > 100:
                logger.error(
                    f"FlowFile {flowfile.uuid} has visited processor {proc_id} "
                    f"{flowfile_processor_visits[visit_key]} times. Breaking cycle."
                )
                continue

            try:
                # Execute processor
                result = processor_func(flowfile)

                # Route outputs to next processors
                has_connections = False
                for relationship, output_files_list in result.items():
                    if proc_id in CONNECTIONS:
                        for conn_rel, next_proc in CONNECTIONS[proc_id]:
                            if conn_rel == relationship:
                                has_connections = True
                                queue.append((next_proc, output_files_list))
                                logger.debug(
                                    f"Routing {len(output_files_list)} flowfile(s) "
                                    f"via '{relationship}' to {next_proc}"
                                )

                    # If no connections for this relationship, it's a terminal output
                    if not has_connections:
                        output_files.extend(output_files_list)
                        logger.debug(
                            f"Terminal output: {len(output_files_list)} flowfile(s) "
                            f"via '{relationship}'"
                        )

            except Exception as e:
                logger.error(
                    f"Processor {proc_id} ({processor_meta.get('name', 'unknown')}) "
                    f"failed: {e}", exc_info=True
                )
                # Check if failure relationship exists
                if proc_id in CONNECTIONS:
                    for conn_rel, next_proc in CONNECTIONS[proc_id]:
                        if conn_rel.lower() == "failure":
                            queue.append((next_proc, [flowfile]))
                            logger.info(f"Routing to failure relationship: {next_proc}")
                            break
                else:
                    # No failure handling, add to outputs
                    output_files.append(flowfile)

    logger.info(f"Flow execution complete. Total iterations: {iteration}, Output files: {len(output_files)}")
    return output_files


def get_source_processors() -> List[str]:
    """
    Get processor IDs that are flow sources (no incoming connections).

    Returns:
        List of source processor IDs
    """
    all_processors = set(PROCESSOR_MAP.keys())
    destination_processors = set()

    for connections in CONNECTIONS.values():
        for _, dest_id in connections:
            destination_processors.add(dest_id)

    return list(all_processors - destination_processors)


def get_sink_processors() -> List[str]:
    """
    Get processor IDs that are flow sinks (no outgoing connections).

    Returns:
        List of sink processor IDs
    """
    all_processors = set(PROCESSOR_MAP.keys())
    source_processors = set(CONNECTIONS.keys())

    return list(all_processors - source_processors)


# ============================================================================
# Entry Points
# ============================================================================

def main():
    """Example usage of the generated flow."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # Get source processors
    sources = get_source_processors()

    if not sources:
        logger.error("No source processors found. Cannot start flow.")
        return

    logger.info(f"Found {len(sources)} source processor(s): {sources}")

    # Create initial flowfile
    flowfile = FlowFile(
        content=b"test data",
        attributes={"source": "generated_flow", "timestamp": datetime.now().isoformat()}
    )

    logger.info(f"Starting flow from processor: {sources[0]}")

    # Run flow from first source processor
    try:
        results = run_flow([flowfile], sources[0])

        logger.info(f"Flow complete. Output files: {len(results)}")
        for i, ff in enumerate(results):
            logger.info(
                f"  Output {i+1}: UUID={ff.uuid[:8]}..., "
                f"Size={ff.size} bytes, Attributes={len(ff.attributes)} keys"
            )
            logger.debug(f"    Attributes: {ff.attributes}")
    except Exception as e:
        logger.error(f"Flow execution failed: {e}", exc_info=True)


if __name__ == "__main__":
    main()
